{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOb0PTxfk+N/mu0bgp5pKA3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XiaoshanZhou624/velocity-replay/blob/main/Contrastive_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnkRIE8kshCw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "\n",
        "# Gaussian smoothing function\n",
        "def gaussian_smooth(data, sigma=1):\n",
        "    return gaussian_filter1d(data, sigma=sigma)\n",
        "\n",
        "# Autonomous system dynamics simulation\n",
        "def generate_system_speeds(time, dt, a=0.5, initial_speed=0):\n",
        "    system_speeds = np.zeros_like(time)\n",
        "    system_speeds[0] = initial_speed\n",
        "    for i in range(1, len(time)):\n",
        "        system_speeds[i] = system_speeds[i-1] + a * dt\n",
        "    return system_speeds\n",
        "\n",
        "# Generate human input based on Bayesian updating\n",
        "def generate_human_preferred_speeds(time, system_speeds, initial_speed=0):\n",
        "    human_changes = np.random.normal(0, 0.1, size=time.shape)\n",
        "    human_input = np.cumsum(human_changes)\n",
        "    human_input = 2 * (human_input - np.min(human_input)) / (np.max(human_input) - np.min(human_input)) - 1\n",
        "    human_input = np.abs(human_input)  # Make all values positive\n",
        "\n",
        "    human_preferred_speeds = np.zeros_like(time)\n",
        "    human_preferred_speeds[0] = initial_speed + human_input[0]\n",
        "    for i in range(1, len(time)):\n",
        "        prior_mean = 0.5 * system_speeds[i] + 0.5 * human_preferred_speeds[i-1]\n",
        "        human_preferred_speeds[i] = (2 * prior_mean + human_input[i]) / 2\n",
        "\n",
        "    return human_preferred_speeds\n",
        "\n",
        "# Time setup\n",
        "def generate_time(T=10, dt=0.1):\n",
        "    return np.arange(0, T + dt, dt)\n",
        "\n",
        "\n",
        "# Modified Dataset Class with Sliding Window\n",
        "class SimulatedSpeedDataset(Dataset):\n",
        "    def __init__(self, window_size=8):\n",
        "        self.time = generate_time()\n",
        "        self.system_speeds = generate_system_speeds(self.time, dt=0.1)\n",
        "        self.human_preferred_speeds = generate_human_preferred_speeds(self.time, self.system_speeds)\n",
        "        self.window_size = window_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.time) - self.window_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get window of speeds\n",
        "        system_speed_window = self.system_speeds[idx:idx + self.window_size]\n",
        "        human_speed_window = self.human_preferred_speeds[idx:idx + self.window_size]\n",
        "\n",
        "        # Apply Gaussian smoothing to the windows\n",
        "        system_speed_window = gaussian_smooth(system_speed_window, sigma=1)\n",
        "        human_speed_window = gaussian_smooth(human_speed_window, sigma=1)\n",
        "\n",
        "        # Generate positive pair (next window in sequence)\n",
        "        positive_idx = (idx + 1) % (len(self.time) - self.window_size)\n",
        "        positive_window = self.human_preferred_speeds[positive_idx:positive_idx + self.window_size]\n",
        "        positive_window = gaussian_smooth(positive_window, sigma=1)\n",
        "\n",
        "        # Generate negative pair (random window)\n",
        "        negative_idx = np.random.randint(0, len(self.time) - self.window_size)\n",
        "        negative_window = self.human_preferred_speeds[negative_idx:negative_idx + self.window_size]\n",
        "        negative_window = gaussian_smooth(negative_window, sigma=1)\n",
        "\n",
        "        # Convert to torch tensors\n",
        "        return (torch.tensor(system_speed_window, dtype=torch.float32),\n",
        "                torch.tensor(human_speed_window, dtype=torch.float32),\n",
        "                torch.tensor(positive_window, dtype=torch.float32),\n",
        "                torch.tensor(negative_window, dtype=torch.float32))\n",
        "\n",
        "\n",
        "class SiameseNetworkGRU(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=32, num_layers=1, window_size=3):\n",
        "        super(SiameseNetworkGRU, self).__init__()\n",
        "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)  # GRU Layer\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hidden_size, 16),  # Reduce the hidden size output\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 8)\n",
        "        )\n",
        "\n",
        "    def forward(self, input1, input2):\n",
        "        # GRU expects input of shape (batch_size, seq_len, input_size)\n",
        "        output1, _ = self.gru(input1)\n",
        "        output2, _ = self.gru(input2)\n",
        "\n",
        "        # Use the last output of the GRU (output1[:,-1,:])\n",
        "        output1 = self.fc(output1[:, -1, :])\n",
        "        output2 = self.fc(output2[:, -1, :])\n",
        "\n",
        "        return output1, output2\n",
        "\n",
        "    def embedding(self, x):\n",
        "        output, _ = self.gru(x)\n",
        "        return self.fc(output[:, -1, :])\n",
        "\n",
        "\n",
        "def contrastive_loss(output1, output2, label, margin=1.0, eps=1e-6):\n",
        "    # Compute the Euclidean distance between two windows of embeddings\n",
        "    euclidean_distance = ((output1 - output2).pow(2).sum(1) + eps).sqrt()\n",
        "\n",
        "    # Calculate the loss for positive and negative pairs\n",
        "    loss_positive = (1 - label) * euclidean_distance.pow(2)\n",
        "    loss_negative = label * torch.clamp(margin - euclidean_distance, min=0.0).pow(2)\n",
        "    loss_contrastive = torch.mean(loss_positive + loss_negative)\n",
        "\n",
        "    return loss_contrastive\n",
        "\n",
        "def train(model, dataloader, epochs=10, lr=0.01, window_size=4):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr) # weight_decay=1e-6\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.99, patience=5)\n",
        "    model.train()\n",
        "    epoch_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        for system_speed_window, human_speed_window, positive_window, negative_window in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Reshape to (batch_size, window_size, input_size=1)\n",
        "            human_speed_window = human_speed_window.unsqueeze(-1)  # (batch_size, window_size) -> (batch_size, window_size, 1)\n",
        "            system_speed_window = system_speed_window.unsqueeze(-1)\n",
        "            positive_window = positive_window.unsqueeze(-1)\n",
        "            negative_window = negative_window.unsqueeze(-1)\n",
        "\n",
        "            # Compute embeddings for windows\n",
        "            human_emb, system_emb = model(human_speed_window, system_speed_window)\n",
        "            positive_emb, _ = model(positive_window, system_speed_window)\n",
        "            negative_emb, _ = model(negative_window, system_speed_window)\n",
        "\n",
        "            # Calculate loss for positive and negative pairs\n",
        "            positive_loss = contrastive_loss(human_emb, positive_emb, torch.zeros(human_emb.size(0), dtype=torch.float32))\n",
        "            negative_loss = contrastive_loss(human_emb, negative_emb, torch.ones(human_emb.size(0), dtype=torch.float32))\n",
        "\n",
        "            loss = positive_loss + negative_loss\n",
        "            loss.backward()\n",
        "            # Apply gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "        average_loss = total_loss / num_batches\n",
        "        epoch_losses.append(average_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Average Loss: {average_loss}\")\n",
        "\n",
        "        scheduler.step(average_loss)\n",
        "\n",
        "    return epoch_losses\n",
        "\n",
        "# Initialize dataset and model\n",
        "dataset = SimulatedSpeedDataset()\n",
        "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "model = SiameseNetworkGRU()\n",
        "# train(model, dataloader)\n",
        "\n",
        "# Train the model\n",
        "epoch_losses = train(model, dataloader, epochs=100)\n",
        "\n",
        "# Plotting the training loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(epoch_losses, label='Training Loss')\n",
        "plt.title('Training Loss Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Average Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "def visualize_embeddings(model, dataloader):\n",
        "    model.eval()\n",
        "    embeddings, labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for system_speed, human_speed, _, _ in dataloader:\n",
        "            # Reshape to 3D tensor: (batch_size, sequence_length=1, input_size=1)\n",
        "            system_speed = system_speed.unsqueeze(-1)  # Adding the third dimension\n",
        "            human_speed = human_speed.unsqueeze(-1)\n",
        "\n",
        "            # Get embeddings for system speed\n",
        "            emb = model.embedding(system_speed).squeeze(0).numpy()  # Remove batch dimension\n",
        "            embeddings.append(emb)\n",
        "            labels.append('System Speed')\n",
        "\n",
        "            # Get embeddings for human speed\n",
        "            emb = model.embedding(human_speed).squeeze(0).numpy()  # Remove batch dimension\n",
        "            embeddings.append(emb)\n",
        "            labels.append('Human Speed')\n",
        "\n",
        "    # Stack all embeddings and reduce dimensions using t-SNE\n",
        "    embeddings = np.vstack(embeddings)\n",
        "    embeddings_reduced = TSNE(n_components=2).fit_transform(embeddings)  # Using t-SNE for dimensionality reduction\n",
        "\n",
        "    # Plot the reduced embeddings\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    for i, label in enumerate(np.unique(labels)):\n",
        "        indices = [j for j, x in enumerate(labels) if x == label]\n",
        "        plt.scatter(embeddings_reduced[indices, 0], embeddings_reduced[indices, 1], label=label, alpha=0.5)\n",
        "    plt.legend()\n",
        "    plt.title('t-SNE of System and Human Speed Embeddings')\n",
        "    plt.xlabel('t-SNE Feature 1')\n",
        "    plt.ylabel('t-SNE Feature 2')\n",
        "    plt.show()\n",
        "\n",
        "# Assuming `dataset` is your SimulatedSpeedDataset instance\n",
        "visualize_embeddings(model, DataLoader(dataset, batch_size=1, shuffle=False))\n",
        "\n",
        "\n",
        "class VelocityRegressionModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(VelocityRegressionModel, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_dim, 16),  # Make sure input_dim matches the embedding size\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 1)  # Output is the predicted velocity (1 value)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "def train_regression_model(model, dataloader, regression_model, num_epochs=100, lr=0.001):\n",
        "    optimizer = torch.optim.Adam(regression_model.parameters(), lr=lr)\n",
        "    criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
        "    epoch_losses = []  # List to store loss for each epoch\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        for system_speed, human_speed, _, _ in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Prepare inputs\n",
        "            system_speed = system_speed.unsqueeze(-1)\n",
        "            human_speed = human_speed.unsqueeze(-1)\n",
        "\n",
        "            # Get embeddings from the model\n",
        "            system_emb = model.embedding(system_speed).squeeze(0)\n",
        "            human_emb = model.embedding(human_speed).squeeze(0)\n",
        "\n",
        "            # Predict velocities from embeddings using the regression model\n",
        "            predicted_system_speed = regression_model(system_emb)\n",
        "            predicted_human_speed = regression_model(human_emb)\n",
        "\n",
        "            # Loss between predicted and actual velocities\n",
        "            loss_system = criterion(predicted_system_speed, system_speed.squeeze(-1))\n",
        "            loss_human = criterion(predicted_human_speed, human_speed.squeeze(-1))\n",
        "            loss = loss_system + loss_human\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "        # Calculate average loss for the epoch\n",
        "        avg_loss = total_loss / num_batches\n",
        "        epoch_losses.append(avg_loss)  # Store the average loss for this epoch\n",
        "\n",
        "        print(f'Epoch {epoch + 1}, Loss: {avg_loss}')\n",
        "\n",
        "    # Plot the loss over epochs\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(range(1, num_epochs + 1), epoch_losses, label='Training Loss', color='blue')\n",
        "    plt.title('Training Loss Over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def visualize_velocity_comparison(model, regression_model, dataloader):\n",
        "    model.eval()\n",
        "    regression_model.eval()\n",
        "    actual_system_speeds, predicted_system_speeds = [], []\n",
        "    actual_human_speeds, predicted_human_speeds = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for system_speed, human_speed, _, _ in dataloader:\n",
        "            # Prepare inputs\n",
        "            system_speed = system_speed.unsqueeze(-1)\n",
        "            human_speed = human_speed.unsqueeze(-1)\n",
        "\n",
        "            # Get embeddings from the model\n",
        "            system_emb = model.embedding(system_speed).squeeze(0)\n",
        "            human_emb = model.embedding(human_speed).squeeze(0)\n",
        "\n",
        "            # Predict velocities from embeddings using the regression model\n",
        "            predicted_system_speed = regression_model(system_emb)\n",
        "            predicted_human_speed = regression_model(human_emb)\n",
        "\n",
        "            # Store actual and predicted values for visualization\n",
        "            actual_system_speeds.append(system_speed.squeeze(-1).numpy())\n",
        "            predicted_system_speeds.append(predicted_system_speed.numpy())\n",
        "            actual_human_speeds.append(human_speed.squeeze(-1).numpy())\n",
        "            predicted_human_speeds.append(predicted_human_speed.numpy())\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    actual_system_speeds = np.concatenate(actual_system_speeds)\n",
        "    predicted_system_speeds = np.concatenate(predicted_system_speeds)\n",
        "    actual_human_speeds = np.concatenate(actual_human_speeds)\n",
        "    predicted_human_speeds = np.concatenate(predicted_human_speeds)\n",
        "\n",
        "    # Plot System Velocity\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(actual_system_speeds, label='Actual System Speed', color='blue')\n",
        "    plt.plot(predicted_system_speeds, label='Predicted System Speed', color='red', linestyle='--')\n",
        "    plt.title('System Speed: Actual vs Predicted')\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Speed')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot Human-Preferred Velocity\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(actual_human_speeds, label='Actual Human Speed', color='blue')\n",
        "    plt.plot(predicted_human_speeds, label='Predicted Human Speed', color='red', linestyle='--')\n",
        "    plt.title('Human-Preferred Speed: Actual vs Predicted')\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Speed')\n",
        "    plt.legend()\n",
        "\n",
        "    # Save the figure if a path is provided\n",
        "    save_path=\"t-SNE.png\"\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
        "        print(f\"Visualization saved as {save_path}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def smooth_connect(predicted_human_speeds, smooth_length=20):\n",
        "    # Create a linear interpolation from 0 to the value at the 20th point\n",
        "    transition_values = np.linspace(0, predicted_human_speeds[smooth_length], smooth_length)\n",
        "\n",
        "    # Replace the first smooth_length points with the transition values\n",
        "    predicted_human_speeds[:smooth_length] = transition_values\n",
        "\n",
        "    # Return the modified array\n",
        "    return predicted_human_speeds\n",
        "\n",
        "\n",
        "def visualize_velocity_comparison_replay(model, regression_model, dataloader):\n",
        "    model.eval()\n",
        "    regression_model.eval()\n",
        "    actual_human_speeds, predicted_human_speeds = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for system_speed, human_speed, _, _ in dataloader:\n",
        "            # Prepare inputs\n",
        "            human_speed = human_speed.unsqueeze(-1)\n",
        "\n",
        "            # Get embeddings from the model\n",
        "            human_emb = model.embedding(human_speed).squeeze(0)\n",
        "\n",
        "            # Predict velocities from embeddings using the regression model\n",
        "            predicted_human_speed = regression_model(human_emb)\n",
        "\n",
        "            # Store actual and predicted values for visualization\n",
        "            actual_human_speeds.append(human_speed.squeeze(-1).numpy())\n",
        "            predicted_human_speeds.append(predicted_human_speed.numpy())\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    actual_human_speeds = np.concatenate(actual_human_speeds)\n",
        "    predicted_human_speeds = np.concatenate(predicted_human_speeds)\n",
        "\n",
        "    # Apply the smooth transition to predicted human speeds\n",
        "    predicted_human_speeds = smooth_connect(predicted_human_speeds, smooth_length=20)\n",
        "\n",
        "    # Plot Human-Preferred Velocity\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(actual_human_speeds, label='Actual Human Speed', color='blue')\n",
        "    plt.plot(predicted_human_speeds, label='Human Predicted Speed', color='red', linestyle='--')\n",
        "    plt.title('Human-Preferred Speed: Actual vs Predicted')\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Speed')\n",
        "    plt.legend(loc='upper left')  # Simplified legend with only two items\n",
        "\n",
        "    # Save the figure if a path is provided\n",
        "    save_path=\"velocity_reproduce.png\"\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
        "        print(f\"Visualization saved as {save_path}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Initialize the regression model with the correct embedding size\n",
        "embedding_size = 8  # Set this to the size of the embeddings produced by the model\n",
        "regression_model = VelocityRegressionModel(input_dim=embedding_size)\n",
        "\n",
        "# Train the regression model\n",
        "train_regression_model(model, DataLoader(dataset, batch_size=1, shuffle=False), regression_model)\n",
        "\n",
        "# visualize_velocity_comparison(model, regression_model, DataLoader(dataset, batch_size=1, shuffle=False))\n",
        "# visualize_velocity_comparison_replay(model, regression_model, DataLoader(dataset, batch_size=1, shuffle=False))\n",
        "visualize_velocity_comparison(model, regression_model, DataLoader(dataset, batch_size=1, shuffle=False))\n"
      ]
    }
  ]
}